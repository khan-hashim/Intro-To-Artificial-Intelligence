{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khan-hashim/Intro-To-Artificial-Intelligence/blob/main/YouTube_Ranker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing libraries"
      ],
      "metadata": {
        "id": "ZbceAhyS-Bu4"
      },
      "id": "ZbceAhyS-Bu4"
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "id": "3b34128d"
      },
      "outputs": [],
      "source": [
        "#Importing the correct functions/libraries\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.linear_model import  Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import set_config\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "id": "3b34128d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loadind the Dataset"
      ],
      "metadata": {
        "id": "29z4fn3V-Fk2"
      },
      "id": "29z4fn3V-Fk2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qSOhov-_HSD"
      },
      "outputs": [],
      "source": [
        "# Try reading the CSV file with different encodings to find the correct one\n",
        "url = \"https://raw.githubusercontent.com/Vishek12/EECS-3401-Project/main/Global_YouTube_Statistics.csv\"\n",
        "encodings_to_try = ['utf-8', 'latin1', 'utf-16']\n",
        "for encoding in encodings_to_try:\n",
        "    try:\n",
        "        youtube_data = pd.read_csv(url, encoding=encoding)\n",
        "        break\n",
        "    except UnicodeDecodeError:\n",
        "        continue\n",
        "\n",
        "print(youtube_data.shape)\n",
        "#Displaying the data\n",
        "\n"
      ],
      "id": "6qSOhov-_HSD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deleting unnecessary columns"
      ],
      "metadata": {
        "id": "Uiy8nNzu-O4H"
      },
      "id": "Uiy8nNzu-O4H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8785c90a-950e-4eab-889e-7cb2f9e9f83a"
      },
      "outputs": [],
      "source": [
        "# Display the shape of the DataFrame before unimportant dropping columns\n",
        "print(youtube_data.shape)\n",
        "\n",
        "columns_to_drop = ['rank','Title','Youtuber','Country','Abbreviation','channel_type','created_date','created_month','Latitude', 'Longitude']\n",
        "youtube_data = youtube_data.drop(columns=columns_to_drop)\n",
        "\n",
        "#Display the truncated data\n",
        "print(youtube_data.shape)\n"
      ],
      "id": "8785c90a-950e-4eab-889e-7cb2f9e9f83a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. EDA on the Dataset"
      ],
      "metadata": {
        "id": "bItHprU4-bdV"
      },
      "id": "bItHprU4-bdV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Histograms to understand the distribution of Data"
      ],
      "metadata": {
        "id": "V9bQiMCW-kvH"
      },
      "id": "V9bQiMCW-kvH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ7Iaz6vn6Qr"
      },
      "outputs": [],
      "source": [
        "youtube_data.hist(figsize=(24, 16))\n"
      ],
      "id": "EZ7Iaz6vn6Qr"
    },
    {
      "cell_type": "code",
      "source": [
        "# As we can see from histogram Created Year has a wrong value so before moving on we will address that issue\n",
        "youtube_data.drop(youtube_data.loc[youtube_data['created_year'] == 1970].index, inplace=True)"
      ],
      "metadata": {
        "id": "hyHKVSeOC4rm"
      },
      "id": "hyHKVSeOC4rm",
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Correlation with regard to our target"
      ],
      "metadata": {
        "id": "RtuHdrcE-yFw"
      },
      "id": "RtuHdrcE-yFw"
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = youtube_data.corr(numeric_only=True)\n",
        "corr_matrix[\"subscribers\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "IDVA5Dalf4zv"
      },
      "id": "IDVA5Dalf4zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Visualizing different coorelation"
      ],
      "metadata": {
        "id": "bBGCQpLT-7Tx"
      },
      "id": "bBGCQpLT-7Tx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nho5cLOFn58C"
      },
      "outputs": [],
      "source": [
        "# Scatter Plot to show correlation between video views and subscribers\n",
        "X = youtube_data[\"video views\"]\n",
        "y = youtube_data[\"subscribers\"]\n",
        "# Plot points\n",
        "fig, pl = plt.subplots(figsize=(20, 10))\n",
        "pl.scatter(X, y, color = 'b')\n",
        "plt.xlabel(\"video views\")\n",
        "plt.ylabel(\"subscribers\")\n"
      ],
      "id": "nho5cLOFn58C"
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter Plot to show correlation between uploads and subscribers\n",
        "X = youtube_data[\"uploads\"]\n",
        "y = youtube_data[\"subscribers\"]\n",
        "# Plot points\n",
        "fig, pl = plt.subplots(figsize=(20, 10))\n",
        "pl.scatter(X, y, color = 'b')\n",
        "plt.xlabel(\"uploads\")\n",
        "plt.ylabel(\"subscribers\")\n"
      ],
      "metadata": {
        "id": "d_3g6F9ru-ij"
      },
      "id": "d_3g6F9ru-ij",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Line Graph between population and subscribers\n",
        "PopulationVsSubscribers = sns.lineplot(x=\"Population\", y=\"subscribers\", data=youtube_data, errorbar=None)"
      ],
      "metadata": {
        "id": "CEBtvVFNuklk"
      },
      "id": "CEBtvVFNuklk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Line Graph between year channel was created and subscribers\n",
        "CreatedYearVsSubscribers = sns.lineplot(x=\"created_year\", y=\"subscribers\", data=youtube_data, errorbar=None)"
      ],
      "metadata": {
        "id": "oWQqB8vVwt42"
      },
      "id": "oWQqB8vVwt42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preparing the data for Machine Learning Algorithms"
      ],
      "metadata": {
        "id": "l97zdvet_ud1"
      },
      "id": "l97zdvet_ud1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84798508-1c93-4abc-894c-8d1405124073"
      },
      "outputs": [],
      "source": [
        "#Pipeline the data so that we can remove duplicate rows and fill empty values\n",
        "\n",
        "#Duplicate entries in the code\n",
        "print(\"Data Duplicates before transformation pipelines:\")\n",
        "duplicates = youtube_data.duplicated().sum() #Expected 0\n",
        "print(duplicates)\n",
        "#Just in case we do have duplicates\n",
        "if(youtube_data.duplicated().sum() > 0):\n",
        "    youtube_data.drop_duplicates(inplace=True)\n",
        "\n",
        "\n",
        "# Assuming youtube_data is your DataFrame\n",
        "\n",
        "# Find the number of missing entries in the dataset\n",
        "missing_values = youtube_data.isna().sum()\n",
        "print(\"\\nMissing Values Before: \")\n",
        "print(missing_values)\n"
      ],
      "id": "84798508-1c93-4abc-894c-8d1405124073"
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling target column to get cleaner results from the models\n",
        "youtube_data['subscribers_scaled by 1,000,000'] = youtube_data['subscribers'] / 1_000_000\n",
        "youtube_data.drop('subscribers', axis=1, inplace=True)\n",
        "youtube_data.head()"
      ],
      "metadata": {
        "id": "i7hEme8sk7SG"
      },
      "id": "i7hEme8sk7SG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh6C6xUY5mFd"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into number features and categorical features\n",
        "num_cols = youtube_data.select_dtypes(include='number').columns.to_list()\n",
        "cat_cols = youtube_data.select_dtypes(exclude='number').columns.to_list()\n",
        "num_cols.remove(\"subscribers_scaled by 1,000,000\")\n",
        "\n",
        "# Create pipelines for numeric and categorical columns\n",
        "num_pipeline = make_pipeline(SimpleImputer(strategy='mean'), StandardScaler())\n",
        "cat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(sparse=False))\n",
        "\n",
        "\n",
        "# Set the estimators\n",
        "preprocessing = ColumnTransformer([('num', num_pipeline, num_cols),\n",
        "                                   ('cat', cat_pipeline, cat_cols)],\n",
        "                                    remainder='passthrough')\n",
        "\n",
        "preprocessing"
      ],
      "id": "gh6C6xUY5mFd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad7be02d-175b-4e30-b99e-7823fd0d9e8b"
      },
      "outputs": [],
      "source": [
        "# Running our data through the pipeline\n",
        "youtube_prepped = preprocessing.fit_transform(youtube_data)\n",
        "\n",
        "# # # Concatenating the feature names for our prepped data\n",
        "\n",
        "feature_names=preprocessing.get_feature_names_out()\n",
        "youtube_prepped = pd.DataFrame(data=youtube_prepped, columns=feature_names)\n",
        "\n",
        "\n",
        "print(youtube_prepped.shape)\n",
        "youtube_prepped.head()"
      ],
      "id": "ad7be02d-175b-4e30-b99e-7823fd0d9e8b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zREJ63fj8AfK"
      },
      "outputs": [],
      "source": [
        "# # Double Checking if all missing features have been taken care of\n",
        "missing_values = youtube_prepped.isna().sum()\n",
        "print(\"\\nMissing Values After: \")\n",
        "missing_values"
      ],
      "id": "zREJ63fj8AfK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f663c8fd-ef41-4895-b0b1-1d207d4b3c63"
      },
      "outputs": [],
      "source": [
        "#Split the Dataset into 80% for training and 20% for testing\n",
        "\n",
        "X = youtube_prepped.drop(['remainder__subscribers_scaled by 1,000,000'], axis=1)\n",
        "y = youtube_prepped['remainder__subscribers_scaled by 1,000,000']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n"
      ],
      "id": "f663c8fd-ef41-4895-b0b1-1d207d4b3c63"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Training Machine Learning Algorithms"
      ],
      "metadata": {
        "id": "Z0DnUdKi6JBk"
      },
      "id": "Z0DnUdKi6JBk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Linear Regression"
      ],
      "metadata": {
        "id": "avsAwH3e6TZ6"
      },
      "id": "avsAwH3e6TZ6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5P0kzHHAGsT"
      },
      "outputs": [],
      "source": [
        "# Training the model with Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "lr_model.fit(X_train,y_train)"
      ],
      "id": "w5P0kzHHAGsT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb4lGalcAJey"
      },
      "outputs": [],
      "source": [
        "# Testing the model on test set and reporting Mean Absolute Error\n",
        "lr_y_predict = lr_model.predict(X_test)\n",
        "\n",
        "lr_mae = mae(y_test, lr_y_predict)\n",
        "\n",
        "print(lr_mae)"
      ],
      "id": "Vb4lGalcAJey"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Lasso Regression"
      ],
      "metadata": {
        "id": "lIBjfIY67FJn"
      },
      "id": "lIBjfIY67FJn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with Lasso Regression\n",
        "\n",
        "LassoRegression = Lasso(alpha=1)\n",
        "lasso_model = LassoRegression.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "auwl1DrGTD4F"
      },
      "id": "auwl1DrGTD4F",
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model on test set and reporting Mean Absolute Error\n",
        "Lasso_y_predict = lasso_model.predict(X_test)\n",
        "\n",
        "lasso_mae = mae(y_test, Lasso_y_predict)\n",
        "\n",
        "print(f'Lasso Regression MAE: {lasso_mae}')"
      ],
      "metadata": {
        "id": "Fy5vYVMQTKRJ"
      },
      "id": "Fy5vYVMQTKRJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Polynomial with Ridge Regularization"
      ],
      "metadata": {
        "id": "zTWzyFTi7q5F"
      },
      "id": "zTWzyFTi7q5F"
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "metadata": {
        "id": "U7R3oP2SFjwy"
      },
      "outputs": [],
      "source": [
        "# add new features according to the Polynomial degree\n",
        "poly = PolynomialFeatures(degree= 2)\n",
        "X_train_trans = poly.fit_transform(X_train)\n",
        "X_test_trans = poly.transform(X_test)"
      ],
      "id": "U7R3oP2SFjwy"
    },
    {
      "cell_type": "code",
      "source": [
        "# train the ridge model with the new features\n",
        "\n",
        "RidgeRegression = Ridge(alpha=1)\n",
        "ridge_model = RidgeRegression.fit(X_train_trans, y_train)\n"
      ],
      "metadata": {
        "id": "Dqcq5bRxkNTs"
      },
      "id": "Dqcq5bRxkNTs",
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model on test set and reporting Mean Absolute Error\n",
        "\n",
        "ridge_scores = cross_val_score(ridge_model, X_train_trans, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
        "\n",
        "mean_score = -scores.mean()  # Take the negative value to get the mean squared error\n",
        "\n",
        "print(f'Cross-Validation Mean Score: {mean_score}')\n",
        "\n"
      ],
      "metadata": {
        "id": "MAuefiYFkPZm"
      },
      "id": "MAuefiYFkPZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 Graphs for the Best performing algorithms\n"
      ],
      "metadata": {
        "id": "Wn54lTPkaaZM"
      },
      "id": "Wn54lTPkaaZM"
    },
    {
      "cell_type": "code",
      "source": [
        "lr_y_predict = lr_model.predict(X_test)  # X is your feature data\n",
        "plt.scatter(lr_y_predict, y_test)  # y is your actual target values\n",
        "plt.xlabel(\"Subscribers Predicted Values\")\n",
        "plt.ylabel(\"Subscribers Actual Values\")\n",
        "plt.title(\"Predicted vs. Actual Values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3LjK2V2udXTx"
      },
      "id": "3LjK2V2udXTx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}